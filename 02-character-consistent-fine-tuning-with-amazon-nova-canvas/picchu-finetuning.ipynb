{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df9e00cd-3e08-460f-905d-ece622b02369",
   "metadata": {},
   "source": [
    "## Nova Canvas Fine-Tuning for Character Consistency\n",
    "\n",
    "This notebook demonstrates how to fine-tune Amazon Nova Canvas to create character-consistent storyboards using images from the animated short film \"Picchu\".\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll walk through the process of fine-tuning Amazon Nova Canvas to maintain visual consistency for specific characters (Mayu and her mom) across multiple generated images. This approach allows for more precise control over character appearance than prompt engineering alone.\n",
    "\n",
    "### Prerequisites\n",
    "- AWS account with access to Amazon Bedrock\n",
    "- Appropriate IAM permissions for Bedrock, S3, and related services\n",
    "- This notebook must be run in the `us-east-1` AWS region\n",
    "\n",
    "### What You'll Learn\n",
    "- How to prepare training data from existing character images\n",
    "- How to configure and run a fine-tuning job on Amazon Nova Canvas\n",
    "- How to generate character-consistent storyboard frames with your fine-tuned model\n",
    "\n",
    "You can watch the original \"Picchu\" animated short film here: [Picchu on YouTube](https://www.youtube.com/watch?v=XfyJbkRV_Eo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19299fb3-b4de-42ed-bd32-91343d8bda35",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First, let's install the required dependencies for this notebook. These packages will help us with image processing, AWS interactions, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ded209",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ea8b04-2042-4590-b25a-53682f530f53",
   "metadata": {},
   "source": [
    "### Initialize\n",
    "\n",
    "Now we'll set up our AWS environment by initializing the necessary clients and defining key variables. This includes:\n",
    "- Setting up boto3 clients for Bedrock, S3, IAM, and STS\n",
    "- Defining our S3 bucket and prefix for storing training data\n",
    "- Setting the base model ID for fine-tuning\n",
    "- Creating a directory for our training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74679aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba58e94-1fc4-40c5-bf1e-80270f883929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.utils import name_from_base\n",
    "import time\n",
    "import json\n",
    "from image_processing import process_folders, upload_to_s3\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sess.default_bucket() # Set a default S3 bucket or use your own\n",
    "prefix = \"picchu-canvas/images\"\n",
    "\n",
    "\n",
    "# Initialize Boto3 Clients\n",
    "bedrock = boto3.client('bedrock')\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "s3 = boto3.client('s3')\n",
    "iam_client = boto3.client('iam')\n",
    "sts_client = boto3.client('sts')\n",
    "\n",
    "# Account and region info\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "\n",
    "# Base model id for fine-tuning\n",
    "model_id = 'amazon.nova-canvas-v1:0'\n",
    "\n",
    "image_dir = \"picchu_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a31da6-6005-4144-b5a7-09715d7af104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_replace_folder(folder_path):\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)  # Remove the existing folder and its contents\n",
    "    os.makedirs(folder_path)         # Create a new, empty folder\n",
    "\n",
    "create_or_replace_folder(image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73913c20-a6a9-438d-980b-aae423413e5f",
   "metadata": {},
   "source": [
    "## Download images\n",
    "\n",
    "In this step, we'll download a pre-prepared set of images from the \"Picchu\" animated short film. These images will serve as our training data for fine-tuning the Nova Canvas model to consistently generate the main character Mayu and her mom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c58b1c-c82b-4089-bf0f-f93124030aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate https://ws-assets-prod-iad-r-iad-ed304a55c2ca1aee.s3.us-east-1.amazonaws.com/3c3519c9-93dc-404d-87f7-4d9bde05f265/picchu_images.zip\n",
    "\n",
    "!unzip picchu_images.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f2681-aa0e-4b9c-8d20-3f973d6c90f1",
   "metadata": {},
   "source": [
    "### Prepare the images for fine tuning\n",
    "\n",
    "Now we'll process the downloaded images and prepare them for fine-tuning. This function will:\n",
    "1. Upload the images to our S3 bucket\n",
    "2. Generate a manifest file that pairs each image with a descriptive caption\n",
    "3. The manifest file follows the required format for Amazon Nova Canvas fine-tuning\n",
    "\n",
    "For more information on manifest file requirements, see the [Amazon Bedrock documentation on fine-tuning](https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e72237-44a0-47e5-936d-8595ab3bdfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_data = process_folders(image_dirs, bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee483e6-e5e2-4bcd-b9df-224c7790b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = f'{prefix.split(\"-\")[0]}_manifest.jsonl'\n",
    "with open(output_file, 'w') as f:\n",
    "    for item in updated_data:\n",
    "        item_filtered = {d:item[d] for d in item if d != 'id'}\n",
    "        f.write(json.dumps(item_filtered) + '\\n')\n",
    "print(f\"{output_file} processed completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc1a2f5-bd0a-4e28-b15d-895a35e925c6",
   "metadata": {},
   "source": [
    "### Preview the manifest file\n",
    "\n",
    "Let's examine the first few entries of our manifest file to understand its structure. Each line contains a JSON object with:\n",
    "- `image-ref`: The S3 path to the image\n",
    "- `caption`: A detailed description of the image that helps the model learn the character's appearance and style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bdf628-e8cb-4d19-b6f6-1fd06ef17f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 5 {output_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4308de78-396d-46d9-a409-d72ef1b322f3",
   "metadata": {},
   "source": [
    "### Upload manifest to S3\n",
    "\n",
    "Now we'll upload our manifest file to S3. This file will be used by the fine-tuning job to locate and process our training images along with their captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a69b73-c636-471d-865b-768a1cf90072",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = upload_to_s3(output_file, bucket, prefix.replace(\"images\", \"manifests\"))\n",
    "training_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b4130-f651-46de-a948-91b94e209447",
   "metadata": {},
   "source": [
    "## Train Custom Model Using Bedrock\n",
    "\n",
    "Now we'll begin the process of fine-tuning the Amazon Nova Canvas model using our prepared training data. This involves several steps:\n",
    "1. Creating the necessary IAM roles and policies\n",
    "2. Configuring the fine-tuning job parameters\n",
    "3. Submitting the job to Amazon Bedrock\n",
    "4. Monitoring the job progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d704e-562c-4bfd-a7e9-752a2ab34d5d",
   "metadata": {},
   "source": [
    "### Fine tune job preparation - Creating role and policies requirements\n",
    "\n",
    "We will now prepare the necessary IAM role for the fine-tune job. This includes creating the policies required to run customization jobs with Amazon Bedrock.\n",
    "\n",
    "### Create Trust relationship\n",
    "This JSON object defines the trust relationship that allows the Bedrock service to assume a role that will give it the ability to interact with other required AWS services. The conditions set restrict the assumption of the role to a specific account ID and a specific component of the Bedrock service (model_customization_jobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c21cd89-b456-45a0-9b5a-6c7891a52ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This JSON object defines the trust relationship that allows the bedrock service to assume a role that will give it the ability to talk to other required AWS services. The conditions set restrict the assumption of the role to a specfic account ID and a specific component of the bedrock service (model_customization_jobs)\n",
    "ROLE_DOC = f\"\"\"{{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {{\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {{\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            }},\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {{\n",
    "                \"StringEquals\": {{\n",
    "                    \"aws:SourceAccount\": \"{account_id}\"\n",
    "                }},\n",
    "                \"ArnEquals\": {{\n",
    "                    \"aws:SourceArn\": \"arn:aws:bedrock:{region}:{account_id}:model-customization-job/*\"\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9bfe6e-6329-499e-aa56-4b363584347f",
   "metadata": {},
   "source": [
    "### Create S3 access policy\n",
    "\n",
    "This JSON object defines the permissions for the role we want Bedrock to assume. It allows access to the S3 bucket that contains our fine-tuning datasets and permits specific bucket and object operations necessary for the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01110bb2-7844-488b-a2dd-36e82bccb307",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_POLICY_DOC = f\"\"\"{{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {{\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:AbortMultipartUpload\",\n",
    "                \"s3:DeleteObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:GetBucketAcl\",\n",
    "                \"s3:GetBucketNotification\",\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:PutBucketNotification\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{bucket}\",\n",
    "                \"arn:aws:s3:::{bucket}/*\"\n",
    "            ]\n",
    "        }}\n",
    "    ]\n",
    "}}\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AWS",
   "language": "python",
   "name": "aws_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
